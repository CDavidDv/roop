#!/usr/bin/env python3
"""
Script espec√≠fico para solucionar problemas de ONNX Runtime GPU en Google Colab
"""

import os
import sys
import subprocess

def fix_onnx_gpu():
    """Solucionar problema de ONNX Runtime GPU"""
    print("üîß SOLUCIONANDO PROBLEMA ONNX RUNTIME GPU")
    print("=" * 50)
    
    # Paso 1: Desinstalar onnxruntime CPU
    print("üì¶ Paso 1: Desinstalando ONNX Runtime CPU...")
    try:
        subprocess.run([sys.executable, "-m", "pip", "uninstall", "-y", "onnxruntime"], 
                      capture_output=True, text=True)
        print("‚úÖ ONNX Runtime CPU desinstalado")
    except Exception as e:
        print(f"‚ö†Ô∏è Error: {e}")
    
    # Paso 2: Instalar numpy.typing si falta
    print("üì¶ Paso 2: Verificando numpy.typing...")
    try:
        import numpy.typing
        print("‚úÖ numpy.typing disponible")
    except ImportError:
        print("üì¶ Instalando numpy.typing...")
        try:
            result = subprocess.run([
                sys.executable, "-m", "pip", "install", "numpy>=1.26.0"
            ], capture_output=True, text=True)
            if result.returncode == 0:
                print("‚úÖ numpy actualizado")
            else:
                print(f"‚ö†Ô∏è Error actualizando numpy: {result.stderr}")
        except Exception as e:
            print(f"‚ö†Ô∏è Error: {e}")
    
    # Paso 3: Instalar onnxruntime-gpu espec√≠fico para Colab
    print("üì¶ Paso 3: Instalando ONNX Runtime GPU...")
    try:
        # Forzar desinstalaci√≥n completa
        subprocess.run([sys.executable, "-m", "pip", "uninstall", "-y", "onnxruntime", "onnxruntime-gpu"], 
                      capture_output=True, text=True)
        
        # Instalar versi√≥n espec√≠fica para Colab
        result = subprocess.run([
            sys.executable, "-m", "pip", "install", 
            "onnxruntime-gpu==1.15.1", "--no-cache-dir", "--force-reinstall"
        ], capture_output=True, text=True)
        
        if result.returncode == 0:
            print("‚úÖ ONNX Runtime GPU instalado exitosamente")
        else:
            print(f"‚ùå Error: {result.stderr}")
            # Intentar con versi√≥n alternativa
            print("üì¶ Intentando con versi√≥n alternativa...")
            result2 = subprocess.run([
                sys.executable, "-m", "pip", "install", 
                "onnxruntime-gpu", "--no-cache-dir", "--force-reinstall"
            ], capture_output=True, text=True)
            
            if result2.returncode == 0:
                print("‚úÖ ONNX Runtime GPU instalado (versi√≥n alternativa)")
            else:
                print(f"‚ùå Error con versi√≥n alternativa: {result2.stderr}")
                
    except Exception as e:
        print(f"‚ùå Error: {e}")
    
    # Paso 4: Verificar instalaci√≥n
    print("üì¶ Paso 4: Verificando instalaci√≥n...")
    try:
        import onnxruntime as ort
        print(f"ONNX Runtime version: {ort.__version__}")
        print(f"ONNX Runtime file: {ort.__file__}")
        
        # Verificar si es GPU o CPU de manera m√°s precisa
        ort_path = ort.__file__
        if 'onnxruntime-gpu' in ort_path or 'gpu' in ort_path.lower():
            print("‚úÖ ONNX Runtime GPU instalado correctamente")
        else:
            print("‚ùå ONNX Runtime CPU a√∫n instalado")
            print(f"   Ruta: {ort_path}")
            
        providers = ort.get_available_providers()
        print(f"Providers disponibles: {providers}")
        
        if 'CUDAExecutionProvider' in providers:
            print("‚úÖ CUDA GPU disponible")
            
            # Probar crear una sesi√≥n con CUDA
            try:
                import numpy as np
                # Crear un modelo simple para probar
                import onnx
                from onnx import helper
                
                # Crear un modelo ONNX simple
                X = helper.make_tensor_value_info('X', onnx.TensorProto.FLOAT, [1, 3, 224, 224])
                Y = helper.make_tensor_value_info('Y', onnx.TensorProto.FLOAT, [1, 3, 224, 224])
                
                node = helper.make_node('Identity', ['X'], ['Y'])
                graph = helper.make_graph([node], 'test', [X], [Y])
                model = helper.make_model(graph)
                
                # Probar con CUDA
                session = ort.InferenceSession(model.SerializeToString(), 
                                             providers=['CUDAExecutionProvider'])
                print("‚úÖ Sesi√≥n CUDA funcionando correctamente")
                
            except Exception as e:
                print(f"‚ö†Ô∏è Error en sesi√≥n CUDA: {e}")
        else:
            print("‚ùå CUDA GPU no disponible")
            
    except ImportError as e:
        print(f"‚ùå Error importando ONNX Runtime: {e}")

def test_face_swapper():
    """Probar face swapper despu√©s de la instalaci√≥n"""
    print("\nüé≠ PROBANDO FACE SWAPPER")
    print("=" * 50)
    
    try:
        # Verificar numpy.typing primero
        try:
            import numpy.typing
            print("‚úÖ numpy.typing disponible")
        except ImportError as e:
            print(f"‚ùå Error numpy.typing: {e}")
            return
        
        import roop.processors.frame.face_swapper as face_swapper
        
        print("Cargando modelo de face swapper...")
        swapper = face_swapper.get_face_swapper()
        
        if swapper:
            print("‚úÖ Face swapper cargado exitosamente")
            
            # Verificar proveedores
            if hasattr(swapper, 'providers'):
                print(f"Proveedores del modelo: {swapper.providers}")
                if 'CUDAExecutionProvider' in swapper.providers:
                    print("‚úÖ Face swapper usando GPU")
                else:
                    print("‚ö†Ô∏è Face swapper usando CPU")
            else:
                print("Modelo cargado (no se puede verificar proveedores)")
        else:
            print("‚ùå Error cargando face swapper")
            
    except Exception as e:
        print(f"‚ùå Error probando face swapper: {e}")
        import traceback
        traceback.print_exc()

def main():
    print("üöÄ SOLUCIONADOR ONNX RUNTIME GPU - GOOGLE COLAB")
    print("=" * 60)
    
    # Verificar estado actual
    print("üîç Estado actual:")
    try:
        import onnxruntime as ort
        print(f"ONNX Runtime: {ort.__version__}")
        print(f"Archivo: {ort.__file__}")
        print(f"Providers: {ort.get_available_providers()}")
    except ImportError:
        print("ONNX Runtime no instalado")
    
    # Verificar numpy.typing
    try:
        import numpy.typing
        print("‚úÖ numpy.typing disponible")
    except ImportError:
        print("‚ùå numpy.typing no disponible")
    
    # Preguntar si proceder
    response = input("\n¬øProceder con la instalaci√≥n? (y/n): ")
    
    if response.lower() in ['y', 'yes', 's√≠', 'si']:
        fix_onnx_gpu()
        test_face_swapper()
        
        print("\nüéâ PROCESO COMPLETADO")
        print("=" * 50)
        print("Si el problema persiste, ejecuta:")
        print("python test_gpu_force.py")
    else:
        print("‚ùå Proceso cancelado")

if __name__ == "__main__":
    main() 
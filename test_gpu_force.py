#!/usr/bin/env python3
"""
Script de prueba para verificar que el face swapper est√° usando GPU
"""

import os
import sys
import subprocess

# Configurar variables de entorno para GPU
os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
os.environ['CUDA_VISIBLE_DEVICES'] = '0'

def check_cuda_dependencies():
    """Verificar dependencias de CUDA"""
    print("üîß VERIFICACI√ìN DE DEPENDENCIAS CUDA:")
    print("=" * 50)
    
    # Verificar versi√≥n de CUDA
    try:
        result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)
        if result.returncode == 0:
            print("‚úÖ CUDA Toolkit instalado:")
            print(result.stdout.split('\n')[0])
        else:
            print("‚ùå CUDA Toolkit no encontrado")
    except FileNotFoundError:
        print("‚ùå CUDA Toolkit no encontrado (nvcc no est√° en PATH)")
    
    # Verificar drivers NVIDIA
    try:
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
        if result.returncode == 0:
            print("‚úÖ Drivers NVIDIA funcionando:")
            lines = result.stdout.split('\n')
            for line in lines[:3]:  # Mostrar solo las primeras l√≠neas
                if line.strip():
                    print(f"   {line}")
        else:
            print("‚ùå nvidia-smi fall√≥")
    except FileNotFoundError:
        print("‚ùå nvidia-smi no encontrado")
    
    # Verificar onnxruntime-gpu
    try:
        import onnxruntime as ort
        print(f"‚úÖ ONNX Runtime instalado: {ort.__version__}")
        
        # Verificar si es la versi√≥n GPU
        try:
            import onnxruntime.capi.onnxruntime_pybind11_state as ort_state
            print("‚úÖ ONNX Runtime GPU disponible")
        except ImportError:
            print("‚ùå ONNX Runtime GPU no disponible (¬øinstalaste onnxruntime-gpu?)")
            
    except ImportError:
        print("‚ùå ONNX Runtime no instalado")

def test_gpu_availability():
    """Verificar disponibilidad de GPU"""
    print("\nüîç VERIFICACI√ìN DE GPU:")
    print("=" * 40)
    
    try:
        import onnxruntime as ort
        providers = ort.get_available_providers()
        print(f"ONNX Runtime providers: {providers}")
        
        if 'CUDAExecutionProvider' in providers:
            print("‚úÖ CUDA GPU disponible para ONNX Runtime")
        else:
            print("‚ùå CUDA GPU no disponible para ONNX Runtime")
            
        if 'TensorrtExecutionProvider' in providers:
            print("‚úÖ TensorRT disponible")
        else:
            print("‚ùå TensorRT no disponible")
            
    except Exception as e:
        print(f"‚ùå Error ONNX Runtime: {e}")
    
    try:
        import torch
        print(f"PyTorch CUDA: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"PyTorch GPU: {torch.cuda.get_device_name()}")
            print(f"PyTorch VRAM: {torch.cuda.memory_allocated()/1024**3:.2f}GB")
            print(f"PyTorch VRAM Total: {torch.cuda.get_device_properties(0).total_memory/1024**3:.2f}GB")
    except Exception as e:
        print(f"‚ùå Error PyTorch: {e}")
    
    try:
        import tensorflow as tf
        gpus = tf.config.list_physical_devices('GPU')
        print(f"TensorFlow GPU devices: {len(gpus)}")
        if gpus:
            print(f"TensorFlow GPU: {gpus[0]}")
    except Exception as e:
        print(f"‚ùå Error TensorFlow: {e}")

def test_face_swapper_gpu():
    """Probar que el face swapper usa GPU"""
    print("\nüé≠ PROBANDO FACE SWAPPER CON GPU:")
    print("=" * 40)
    
    try:
        # Importar el m√≥dulo de face swapper
        import roop.processors.frame.face_swapper as face_swapper
        
        # Verificar que el modelo se carga con GPU
        print("Cargando modelo de face swapper...")
        swapper = face_swapper.get_face_swapper()
        
        if swapper:
            print("‚úÖ Face swapper cargado exitosamente")
            # Verificar qu√© proveedores est√° usando
            if hasattr(swapper, 'providers'):
                print(f"Proveedores del modelo: {swapper.providers}")
                
                # Verificar si realmente est√° usando GPU
                if any('CUDA' in provider for provider in swapper.providers):
                    print("‚úÖ GPU CUDA confirmado en uso")
                else:
                    print("‚ùå GPU CUDA no confirmado en uso")
            else:
                print("Modelo cargado (no se puede verificar proveedores)")
        else:
            print("‚ùå Error cargando face swapper")
            
    except Exception as e:
        print(f"‚ùå Error probando face swapper: {e}")
        import traceback
        traceback.print_exc()

def test_face_enhancer_gpu():
    """Probar que el face enhancer usa GPU"""
    print("\n‚ú® PROBANDO FACE ENHANCER CON GPU:")
    print("=" * 40)
    
    try:
        # Importar el m√≥dulo de face enhancer
        import roop.processors.frame.face_enhancer as face_enhancer
        
        # Verificar que el dispositivo se detecta correctamente
        device = face_enhancer.get_device()
        print(f"Dispositivo detectado: {device}")
        
        if device == 'cuda':
            print("‚úÖ Face enhancer configurado para usar GPU")
        elif device == 'mps':
            print("‚úÖ Face enhancer configurado para usar CoreML")
        else:
            print("‚ö†Ô∏è Face enhancer usando CPU")
            
    except Exception as e:
        print(f"‚ùå Error probando face enhancer: {e}")

def test_face_analyser_gpu():
    """Probar que el analizador de rostros usa GPU"""
    print("\nüîç PROBANDO FACE ANALYSER CON GPU:")
    print("=" * 40)
    
    try:
        # Importar el m√≥dulo de face analyser
        import roop.face_analyser as face_analyser
        
        # Verificar que el analizador se carga con GPU
        print("Cargando analizador de rostros...")
        analyser = face_analyser.get_face_analyser()
        
        if analyser:
            print("‚úÖ Analizador de rostros cargado exitosamente")
            # Verificar qu√© proveedores est√° usando
            if hasattr(analyser, 'providers'):
                print(f"Proveedores del analizador: {analyser.providers}")
                
                # Verificar si realmente est√° usando GPU
                if any('CUDA' in provider for provider in analyser.providers):
                    print("‚úÖ GPU CUDA confirmado en uso")
                else:
                    print("‚ùå GPU CUDA no confirmado en uso")
            else:
                print("Analizador cargado (no se puede verificar proveedores)")
        else:
            print("‚ùå Error cargando analizador de rostros")
            
    except Exception as e:
        print(f"‚ùå Error probando face analyser: {e}")

def test_manual_cuda_creation():
    """Probar creaci√≥n manual de CUDAExecutionProvider"""
    print("\nüîß PROBANDO CREACI√ìN MANUAL DE CUDA:")
    print("=" * 40)
    
    try:
        import onnxruntime as ort
        
        # Intentar crear sesi√≥n con CUDA
        print("Intentando crear sesi√≥n ONNX Runtime con CUDA...")
        
        # Crear un modelo simple para probar
        import numpy as np
        from onnx import helper, numpy_helper
        
        # Crear un modelo ONNX simple
        X = helper.make_tensor_value_info('X', helper.TensorProto.FLOAT, [1, 3, 224, 224])
        Y = helper.make_tensor_value_info('Y', helper.TensorProto.FLOAT, [1, 3, 224, 224])
        
        # Crear un nodo de identidad
        node = helper.make_node('Identity', inputs=['X'], outputs=['Y'])
        
        # Crear el grafo
        graph = helper.make_graph([node], 'test', [X], [Y])
        
        # Crear el modelo
        model = helper.make_model(graph)
        
        # Intentar crear sesi√≥n con diferentes configuraciones
        providers_to_try = [
            ['CUDAExecutionProvider'],
            ['CUDAExecutionProvider', 'CPUExecutionProvider'],
            ['TensorrtExecutionProvider', 'CUDAExecutionProvider']
        ]
        
        for providers in providers_to_try:
            try:
                print(f"Probando con proveedores: {providers}")
                
                provider_options = {}
                if 'CUDAExecutionProvider' in providers:
                    provider_options['CUDAExecutionProvider'] = {
                        'device_id': 0,
                        'arena_extend_strategy': 'kNextPowerOfTwo',
                        'gpu_mem_limit': 2 * 1024 * 1024 * 1024,
                        'cudnn_conv_use_max_workspace': '1',
                        'do_copy_in_default_stream': '1',
                    }
                
                session = ort.InferenceSession(
                    model.SerializeToString(),
                    providers=providers,
                    provider_options=provider_options
                )
                
                print(f"‚úÖ Sesi√≥n creada exitosamente con: {session.get_providers()}")
                break
                
            except Exception as e:
                print(f"‚ùå Error con {providers}: {e}")
                continue
                
    except Exception as e:
        print(f"‚ùå Error en prueba manual: {e}")

if __name__ == "__main__":
    print("üöÄ INICIANDO PRUEBAS DE GPU FORZADO")
    print("=" * 50)
    
    # Verificar dependencias de CUDA
    check_cuda_dependencies()
    
    # Verificar disponibilidad de GPU
    test_gpu_availability()
    
    # Probar creaci√≥n manual de CUDA
    test_manual_cuda_creation()
    
    # Probar face swapper
    test_face_swapper_gpu()
    
    # Probar face enhancer
    test_face_enhancer_gpu()
    
    # Probar face analyser
    test_face_analyser_gpu()
    
    print("\nüéâ PRUEBAS COMPLETADAS")
    print("=" * 50) 
#!/usr/bin/env python3
"""
Script para ejecutar ROOP ignorando la detecciÃ³n de versiÃ³n y enfocÃ¡ndose en funcionalidad GPU
"""

import os
import sys
import subprocess

def setup_environment():
    """Configurar variables de entorno para forzar GPU"""
    os.environ['CUDA_VISIBLE_DEVICES'] = '0'
    os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'
    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
    os.environ['OMP_NUM_THREADS'] = '1'
    os.environ['MPLBACKEND'] = 'Agg'
    os.environ['NO_ALBUMENTATIONS_UPDATE'] = '1'
    os.environ['ONNXRUNTIME_PROVIDER_SHARED_LIB'] = '/usr/local/cuda/lib64/libonnxruntime_providers_cuda.so'

def check_gpu_functionality():
    """Verificar funcionalidad GPU sin depender de detecciÃ³n de versiÃ³n"""
    print("ğŸ” VERIFICANDO FUNCIONALIDAD GPU")
    print("=" * 40)
    
    # Verificar GPU fÃ­sica
    try:
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
        if result.returncode == 0:
            print("âœ… GPU NVIDIA detectada")
            print(result.stdout.split('\n')[0])
        else:
            print("âŒ GPU NVIDIA no detectada")
            return False
    except Exception as e:
        print(f"âŒ Error verificando GPU: {e}")
        return False
    
    # Verificar ONNX Runtime
    try:
        result = subprocess.run([
            'roop_env/bin/python', '-c', 
            "import onnxruntime as ort; print('Proveedores:', ort.get_available_providers())"
        ], capture_output=True, text=True)
        
        if result.returncode == 0 and 'CUDAExecutionProvider' in result.stdout:
            print("âœ… CUDAExecutionProvider disponible")
            return True
        else:
            print("âŒ CUDAExecutionProvider NO disponible")
            return False
    except Exception as e:
        print(f"âŒ Error verificando ONNX: {e}")
        return False

def run_roop_with_gpu():
    """Ejecutar ROOP con GPU forzado"""
    
    print("ğŸ¬ EJECUTANDO ROOP CON GPU FORZADO")
    print("=" * 50)
    
    # Configurar entorno
    setup_environment()
    
    # Comando optimizado para GPU
    cmd = [
        'roop_env/bin/python', 'run.py',
        '--source', '/content/DanielaAS.jpg',
        '--target', '/content/112.mp4',
        '-o', '/content/DanielaAS112_gpu_forced.mp4',
        '--frame-processor', 'face_swapper',
        '--execution-provider', 'cuda',
        '--max-memory', '8',
        '--execution-threads', '8',
        '--gpu-memory-wait', '15',
        # Configuraciones de alta calidad
        '--temp-frame-format', 'png',
        '--temp-frame-quality', '0',
        '--output-video-encoder', 'libx264',
        '--output-video-quality', '35',
        '--keep-fps'
    ]
    
    print("âš™ï¸ CONFIGURACIÃ“N:")
    print("   â€¢ GPU: Tesla T4 (15GB)")
    print("   â€¢ Execution Provider: cuda")
    print("   â€¢ Calidad: Alta (configuraciones originales)")
    print("   â€¢ Variables de entorno: Configuradas para GPU")
    print("=" * 50)
    
    try:
        print("ğŸš€ Iniciando procesamiento con GPU forzado...")
        result = subprocess.run(cmd, check=True)
        print("\nâœ… Procesamiento completado exitosamente")
        print("ğŸ“ Archivo generado: /content/DanielaAS112_gpu_forced.mp4")
        return True
    except subprocess.CalledProcessError as e:
        print(f"\nâŒ Error en procesamiento: {e}")
        return False

def run_batch_with_gpu():
    """Ejecutar procesamiento en lote con GPU forzado"""
    
    print("ğŸ¬ PROCESAMIENTO EN LOTE CON GPU FORZADO")
    print("=" * 50)
    
    # Configurar entorno
    setup_environment()
    
    # Comando en lote
    cmd = [
        'roop_env/bin/python', 'run_batch_processing.py',
        '--source', '/content/LilitAS.png',
        '--videos', '/content/62.mp4', '/content/71.mp4', '/content/72.mp4', 
        '/content/74.mp4', '/content/75.mp4', '/content/76.mp4', 
        '/content/77.mp4', '/content/78.mp4', '/content/79.mp4',
        '--output-dir', '/content/resultados_gpu_forced',
        '--execution-threads', '8',
        # Configuraciones de alta calidad
        '--temp-frame-format', 'png',
        '--temp-frame-quality', '0',
        '--output-video-encoder', 'libx264',
        '--output-video-quality', '35',
        '--keep-fps'
    ]
    
    print("âš™ï¸ CONFIGURACIÃ“N EN LOTE:")
    print("   â€¢ GPU: Tesla T4 (15GB)")
    print("   â€¢ Execution Provider: cuda")
    print("   â€¢ Calidad: Alta (configuraciones originales)")
    print("   â€¢ Videos: 9 archivos")
    print("   â€¢ Output: /content/resultados_gpu_forced")
    print("=" * 50)
    
    try:
        print("ğŸš€ Iniciando procesamiento en lote con GPU forzado...")
        result = subprocess.run(cmd, check=True)
        print("\nâœ… Procesamiento en lote completado")
        print("ğŸ“ Archivos generados en: /content/resultados_gpu_forced")
        return True
    except subprocess.CalledProcessError as e:
        print(f"\nâŒ Error en procesamiento: {e}")
        return False

def monitor_gpu_usage():
    """Monitorear uso de GPU durante procesamiento"""
    print("\nğŸ“Š MONITOREO DE GPU")
    print("=" * 30)
    
    try:
        result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.used,memory.total,utilization.gpu,temperature.gpu', '--format=csv,noheader,nounits'], 
                               capture_output=True, text=True)
        
        if result.returncode == 0:
            lines = result.stdout.strip().split('\n')
            for line in lines:
                if line.strip():
                    parts = line.split(', ')
                    if len(parts) >= 5:
                        name, mem_used, mem_total, util, temp = parts
                        print(f"ğŸ® GPU: {name}")
                        print(f"ğŸ“Š VRAM: {mem_used}MB/{mem_total}MB ({int(mem_used)/int(mem_total)*100:.1f}%)")
                        print(f"âš¡ UtilizaciÃ³n: {util}%")
                        print(f"ğŸŒ¡ï¸ Temperatura: {temp}Â°C")
    except Exception as e:
        print(f"âŒ Error monitoreando GPU: {e}")

def main():
    """FunciÃ³n principal"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ROOP con GPU forzado (ignora detecciÃ³n de versiÃ³n)')
    parser.add_argument('--mode', choices=['single', 'batch'], default='single', 
                       help='Modo de procesamiento: single o batch')
    parser.add_argument('--monitor', action='store_true', help='Monitorear GPU')
    
    args = parser.parse_args()
    
    # Verificar funcionalidad GPU
    if not check_gpu_functionality():
        print("âŒ GPU no estÃ¡ funcionando correctamente")
        return
    
    # Monitorear GPU si se solicita
    if args.monitor:
        monitor_gpu_usage()
    
    # Ejecutar procesamiento
    if args.mode == 'single':
        success = run_roop_with_gpu()
    else:
        success = run_batch_with_gpu()
    
    if success:
        print("\nğŸ‰ Â¡PROCESAMIENTO COMPLETADO CON Ã‰XITO!")
        print("âœ… GPU funcionando correctamente")
        print("âœ… Alta calidad de video")
        print("âœ… Sin pixelado")
        print("\nğŸ’¡ NOTA: Ignoramos la detecciÃ³n de versiÃ³n y nos enfocamos en funcionalidad")
    else:
        print("\nâŒ Error en el procesamiento")
        print("ğŸ’¡ Verifica que CUDAExecutionProvider estÃ© disponible")

if __name__ == "__main__":
    main() 
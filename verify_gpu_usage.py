#!/usr/bin/env python3
"""
Script para verificar el uso de GPU durante el procesamiento de face-swapper
"""

import subprocess
import time
import threading
import os
import sys

def get_gpu_memory():
    """Obtener uso de memoria GPU"""
    try:
        result = subprocess.run(
            ["nvidia-smi", "--query-gpu=memory.used,memory.total", "--format=csv,noheader,nounits"],
            capture_output=True, text=True
        )
        if result.returncode == 0:
            lines = result.stdout.strip().split('\n')
            for line in lines:
                if ',' in line:
                    used, total = map(int, line.split(','))
                    return used, total
    except Exception as e:
        print(f"Error obteniendo memoria GPU: {e}")
    return 0, 0

def get_gpu_utilization():
    """Obtener utilizaci√≥n de GPU"""
    try:
        result = subprocess.run(
            ["nvidia-smi", "--query-gpu=utilization.gpu", "--format=csv,noheader,nounits"],
            capture_output=True, text=True
        )
        if result.returncode == 0:
            lines = result.stdout.strip().split('\n')
            for line in lines:
                if line.strip().isdigit():
                    return int(line.strip())
    except Exception as e:
        print(f"Error obteniendo utilizaci√≥n GPU: {e}")
    return 0

def monitor_gpu_during_processing():
    """Monitorear GPU durante procesamiento"""
    print("üìä MONITOREO DE GPU DURANTE PROCESAMIENTO")
    print("=" * 50)
    
    # Configurar variables de entorno
    os.environ['CUDA_VISIBLE_DEVICES'] = '0'
    os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'
    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
    os.environ['OMP_NUM_THREADS'] = '1'
    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
    
    print("Variables de entorno configuradas:")
    for var in ['CUDA_VISIBLE_DEVICES', 'TF_FORCE_GPU_ALLOW_GROWTH', 'TF_CPP_MIN_LOG_LEVEL', 'OMP_NUM_THREADS', 'CUDA_LAUNCH_BLOCKING']:
        print(f"  {var}={os.environ.get(var, 'NO SET')}")
    
    print("\nüîç Verificando estado inicial de GPU...")
    initial_used, initial_total = get_gpu_memory()
    initial_util = get_gpu_utilization()
    
    print(f"Memoria inicial: {initial_used}MB / {initial_total}MB")
    print(f"Utilizaci√≥n inicial: {initial_util}%")
    
    # Verificar onnxruntime
    print("\nüîç Verificando ONNX Runtime...")
    try:
        import onnxruntime as ort
        print(f"Versi√≥n ONNX Runtime: {ort.__version__}")
        providers = ort.get_available_providers()
        print(f"Proveedores disponibles: {providers}")
        
        if 'CUDAExecutionProvider' in providers:
            print("‚úÖ CUDAExecutionProvider disponible")
        else:
            print("‚ùå CUDAExecutionProvider no disponible")
            
    except Exception as e:
        print(f"‚ùå Error verificando ONNX Runtime: {e}")
    
    # Probar face swapper
    print("\nüé≠ Probando face swapper...")
    try:
        import roop.processors.frame.face_swapper as face_swapper
        
        print("Cargando modelo...")
        start_time = time.time()
        swapper = face_swapper.get_face_swapper()
        load_time = time.time() - start_time
        
        print(f"‚úÖ Modelo cargado en {load_time:.2f}s")
        
        if hasattr(swapper, 'providers'):
            print(f"Proveedores del modelo: {swapper.providers}")
            
            if any('CUDA' in provider for provider in swapper.providers):
                print("‚úÖ GPU CUDA confirmado en uso")
            else:
                print("‚ùå GPU CUDA no confirmado")
        else:
            print("‚ö†Ô∏è No se pueden verificar proveedores")
        
        # Verificar memoria despu√©s de cargar modelo
        after_load_used, after_load_total = get_gpu_memory()
        after_load_util = get_gpu_utilization()
        
        print(f"\nMemoria despu√©s de cargar modelo: {after_load_used}MB / {after_load_total}MB")
        print(f"Utilizaci√≥n despu√©s de cargar modelo: {after_load_util}%")
        
        memory_increase = after_load_used - initial_used
        if memory_increase > 0:
            print(f"‚úÖ Memoria GPU aument√≥ en {memory_increase}MB - GPU est√° siendo usado")
        else:
            print(f"‚ùå Memoria GPU no aument√≥ - posible problema")
        
    except Exception as e:
        print(f"‚ùå Error probando face swapper: {e}")
        import traceback
        traceback.print_exc()

def test_simple_cuda_session():
    """Probar sesi√≥n CUDA simple"""
    print("\nüß™ PROBANDO SESI√ìN CUDA SIMPLE")
    print("=" * 50)
    
    try:
        import onnxruntime as ort
        import numpy as np
        from onnx import helper
        
        # Crear modelo simple
        X = helper.make_tensor_value_info('X', helper.TensorProto.FLOAT, [1, 3, 224, 224])
        Y = helper.make_tensor_value_info('Y', helper.TensorProto.FLOAT, [1, 3, 224, 224])
        node = helper.make_node('Identity', inputs=['X'], outputs=['Y'])
        graph = helper.make_graph([node], 'test', [X], [Y])
        model = helper.make_model(graph)
        
        print("Creando sesi√≥n con CUDA...")
        start_time = time.time()
        
        session = ort.InferenceSession(
            model.SerializeToString(),
            providers=['CUDAExecutionProvider', 'CPUExecutionProvider']
        )
        
        creation_time = time.time() - start_time
        print(f"‚úÖ Sesi√≥n creada en {creation_time:.2f}s")
        print(f"Proveedores aplicados: {session.get_providers()}")
        
        # Probar inferencia
        input_data = np.random.randn(1, 3, 224, 224).astype(np.float32)
        
        print("Ejecutando inferencia...")
        start_time = time.time()
        
        output = session.run(['Y'], {'X': input_data})
        
        inference_time = time.time() - start_time
        print(f"‚úÖ Inferencia completada en {inference_time:.4f}s")
        
        # Verificar memoria despu√©s de inferencia
        after_inference_used, after_inference_total = get_gpu_memory()
        after_inference_util = get_gpu_utilization()
        
        print(f"Memoria despu√©s de inferencia: {after_inference_used}MB / {after_inference_total}MB")
        print(f"Utilizaci√≥n despu√©s de inferencia: {after_inference_util}%")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Error probando sesi√≥n CUDA: {e}")
        import traceback
        traceback.print_exc()
        return False

def generate_diagnostic_report():
    """Generar reporte de diagn√≥stico"""
    print("\nüìã GENERANDO REPORTE DE DIAGN√ìSTICO")
    print("=" * 50)
    
    report = []
    report.append("üîß DIAGN√ìSTICO DE GPU PARA FACE-SWAPPER")
    report.append("=" * 60)
    
    # Informaci√≥n del sistema
    try:
        import platform
        report.append(f"Sistema: {platform.system()} {platform.release()}")
    except:
        pass
    
    # Informaci√≥n de GPU
    try:
        result = subprocess.run(["nvidia-smi", "--query-gpu=name,memory.total", "--format=csv,noheader,nounits"], 
                              capture_output=True, text=True)
        if result.returncode == 0:
            gpu_info = result.stdout.strip().split(',')
            if len(gpu_info) >= 2:
                report.append(f"GPU: {gpu_info[0].strip()}")
                report.append(f"VRAM Total: {gpu_info[1].strip()}MB")
    except:
        report.append("GPU: No se pudo obtener informaci√≥n")
    
    # Informaci√≥n de ONNX Runtime
    try:
        import onnxruntime as ort
        report.append(f"ONNX Runtime: {ort.__version__}")
        providers = ort.get_available_providers()
        report.append(f"Proveedores: {providers}")
        
        if 'CUDAExecutionProvider' in providers:
            report.append("‚úÖ CUDA disponible")
        else:
            report.append("‚ùå CUDA no disponible")
    except Exception as e:
        report.append(f"‚ùå Error ONNX Runtime: {e}")
    
    # Estado de memoria
    used, total = get_gpu_memory()
    util = get_gpu_utilization()
    report.append(f"Memoria actual: {used}MB / {total}MB")
    report.append(f"Utilizaci√≥n actual: {util}%")
    
    # Guardar reporte
    with open('diagnostico_gpu.txt', 'w') as f:
        f.write('\n'.join(report))
    
    print("‚úÖ Reporte guardado en 'diagnostico_gpu.txt'")
    
    # Mostrar reporte
    print("\n" + '\n'.join(report))

def main():
    """Funci√≥n principal"""
    print("üîç VERIFICADOR DE USO DE GPU")
    print("=" * 60)
    
    # Generar reporte de diagn√≥stico
    generate_diagnostic_report()
    
    # Monitorear GPU durante procesamiento
    monitor_gpu_during_processing()
    
    # Probar sesi√≥n CUDA simple
    test_simple_cuda_session()
    
    print("\nüéâ VERIFICACI√ìN COMPLETADA")
    print("=" * 60)
    print("üìã RESUMEN:")
    print("- Si ves '‚úÖ GPU CUDA confirmado en uso', el problema est√° resuelto")
    print("- Si ves '‚ùå GPU CUDA no confirmado', ejecuta: python fix_face_swapper_gpu.py")
    print("- Si la memoria GPU no aumenta, hay un problema de configuraci√≥n")
    print("- El archivo 'diagnostico_gpu.txt' contiene informaci√≥n detallada")

if __name__ == "__main__":
    main() 